---
title: "PCA(Principle Component Analysis)" ## 포스트 제목
category:       
    - Machine Learning
tags:           
    - PCA
    - concept
comments:  true
use_math : true
last_modified_at : 2020-09-19
toc: true
# published: false
---

# 목차
  
**1\.** 개념  
**2\.** 차원축소(Dimensionality Reduction)  
    **2\.1\.** 특이치 분해(Singular Value Decomposition, SVD)   
    **2\.2\.** 투영(Projection)   
    **2\.3\.** 차원축소의 장단점 및 이외 방법  
**3\.** 수학적 설명    
**4\.** 참고  
  
---
  
# 1. 개념

- PCA(Principle Component Analysis), 주성분 분석은 고차원 데이터 집합이 주어졌을 때, 원래의 고차원 데이터와 가장 비슷하면서 더 낮은 차원 데이터를 찾아내는 방법이다. 다시 말해, 고차원 데이터의 변화를 잘 설명하는 저차원 데이터로 설명 하는 방법이다. 따라서 `차원축소(dimension reduction)` 라고도 한다.

- PCA에서는 잠재변수(latent variable)과 주어진 측정 데이터가 선형적 관계로 연결되어 있다고 가정한다. 즉 $i$번째 표본의 측정 데이터 벡터 $x_{i}$의 각 원소를 선형조합하면 그 뒤에 숨은 $i$번째 표본의 잠재변수 $u_{i}$ 값을 계산할 수 있다고 가정한다. 이를 수식으로 나타내면 다음과 같다.  

<center>$u_{i} = w^{T} x_{i}$</center>  

- 즉, 측정 데이터들을 행렬$w$ 내의 성분들로 표현할 수 있다는 뜻이다. 이때 행렬$w$ 내 행들을 어떤 벡터 공간에서 선형 생성하는 선형 독립인 벡터로 해석하면 $w$는 측정 데이터들의 기저(basis)가 된다.

- 따라서 `PCA의 핵심은 측정 데이터들의 기저를 찾아 기여도가 높은 기저 벡터들 순으로 데이터들의 정보를 압축하여 표현하는 것이다.`

- 이때, 기저 벡터들과 기여도는 `특이치 분해(Singular Value Decomposition, SVD)` 으로 특이행렬과 특이치로 사용한다.

- 예를 들어, 바리스타들은 자신만의 시그니처 커피를 만들기 위해 원두를 블랜딩하여 커피를 만든다. 블랜딩은 단맛 베이스, 바디감, 강렬함(산미), 3가지 요소의 밸런스를 고려하여 각 요소별 맛이 강한 각종 원산지 커피를 선택하여 진행된다. 이 때, 이 블랜딩 밸런스에 따라 커피맛이 좌우된다. 따라서 바리스타들 마다 만들고 싶은 맛을 3가지 요소의 비율로 조정한다.  

<center>시그니처 커피 = 0.4*단맛 + 0.4*바디감 + 0.2*강렬함</center>  

- 이렇게 바리스타들이 시그니처 커피를 3가지 요소 비율로 표현될 것이라 가정하여 맛을 예상하고 만든다. 차원축소의 관점에서 보면 3차원 데이터를 1차원으로 축소한 것이다.

---

# 2. 차원축소(Dimensionality Reduction)

## 2.1 특이치 분해(Singular Value Decomposition, SVD)

- PCA처럼 고차원 데이터를 저차원으로 변환하는 방법 중 하나는 특이치 분해를 이용하여 입력 데이터의 분산도를 잘 유지시킬 수 있는 저차원 초평면으로 투영시키는 방법이 있다.

- 먼저 임의의 크기, $m \times n$를 가지는 행렬 $A$에 대한 특이치 분해는 아래와 같다.

<center>
$A = U \Sigma V^{T}$
</center>
  
- 이는 임의의 행렬 A를 정보량(특이치)에 따라 여러 정규직교벡터(V, U)로 분해해준다. 
  
<center>
$AV = U \Sigma V_{T} V$  
</center>
  
<center>
$AV = U \Sigma$
</center>
  
- 위와 같이 식을 변형하여 다르게 해석하면, 한 정규직교행렬(기저 벡터 모음) $V$를 행렬 $A$에 의해 선형변환 시킬 때 크기는 대각행렬 $\Sigma$ 내 대각 원소들만큼 변하지만 여전히 직교하는 벡터들 모음 $U$를 찾겠다는 것이다. 즉, 행렬 A를 정보량(특이치)에 따라 여러 정규직교행렬로 분해하여 표현한다는 것이다.

- 여기서 행렬 A의 특성을 가장 잘 반영하는 정보량이 큰 몇개 특이치들과 그에 대응하는 열벡터들로만 행렬 A를 표현하면 이는 곧 정보 압축, 차원축소가 되는 것이다.

---

## 2.2 투영(Projection)

- 위에서 설명했듯, 특이치 분해를 이용하여 PCA 원리에 따라 데이터를 선형적 근사, 차원축소가 가능하다.

- 특이치 분해에서의 로우-랭크 근사(low-rank approximation) 문제와 같다. 이 문제는 다음과 같이 서술된다.    
  
    $N$개의 $M$차원 데이터 벡터 $x_{1}, x_{2}, ... , x_{N}$들을 정규직교인 기저벡터 $w_{1}, w_{2}, ... , w_{K}$로 이루어진 $K$차원 벡터공간으로 투영하여 가장 비슷한 $N$개의 $K$차원 벡터 $x_{1}^{w}, x_{2}^{w}, ... , x_{N}^{w}$를 만들기 위한 정규직교 기저벡터  $w_{1}, w_{2}, ... , w_{K}$를 찾는다.
  
- 다만 원래의 로우-랭크 근사문제와 달리 근사 성능을 높이기 위해 직선이 원점을 지나야 한다는 제한 조건을 없애야 하므로 다음과 같이 변형된다.  
    
    $N$개의 $M$차원 데이터 벡터 $x_{1}, x_{2}, ... , x_{N}$에 대해 어떤 상수 벡터 $x_{0}$를 뺀 데이터 벡터 $x_{1}-x_{0}, x_{2}-x_{0}, ... , x_{N}-x_{0}$를 정규직교 기저벡터  $(w_{1}, w_{2}, ... , w_{K})$로 이루어진 $K$차원 벡터공간으로 투영하여 가장 비슷한 $N$개의 $K$차원 벡터 $x_{1}^{w}, x_{2}^{w}, ... , x_{N}^{w}$를 만들기 위한 정규직교 기저벡터 $w_{1}, w_{2}, ... , w_{K}$와 상수 벡터 $x_{0}$를 찾는다.
  
- $N$개의 데이터를 1차원 직선에 투영하는 문제라고 하면 원점을 지나는 직선을 찾는 것이 아니라 원점이 아닌 어떤 점 $x_{0}$를 지나는 직선을 찾는 문제로 바꾼 것이다.  
  
- 따라서 이 문제의 답은 다음과 같다.  
  
    $x_{0}$는 데이터 벡터 $x_{1}, x_{2}, ... , x_{N}$의 평균벡터이고, $w_{1}, w_{2}, ... , w_{K}$는 가장 큰 $K$개의 특이치에 대응하는 오른쪽 특이벡터 $v_{1}, v_{2}, ... , v_{K}$이다.

---

## 2.3 차원축소의 장단점 및 이외 방법
  
### 1) 차원축소 장단점  

- 장점 :   
    + 쓸모있는 정보만 압축 -> 시스템 훈련 속도 빨라짐  
    + 일부 경우, 잡음이나 불필요한 세부 정보를 걸러내 성능이 높아질 수 있음 (일반적으로는 그렇지 않음)  
    + 데이터 시각화(visualization)에 유용  

- 단점 :  
    + 작업 파이프라인 복잡해져 전체 시스템 관리 어려워짐    
    + 유실되는 정보로 인해 시스템의 성능이 조금 나빠질 수 있음  
  

- 차원축소는 일부 경우, 데이터가 희박(sparse)하여 전체 데이터 특성을 대표하기 어려운 경우, 즉, 차원의 저주가 의심되는 상황에서 성능을 높여줄 수 있다. 따라서 1차적으로는 데이터를 충분하게 확보하는 것이 당연시 되어야한다. 그렇기 때문에 차원축소는 주로 시각화와 훈련 속도 감소 쓰인다.

- 또한, 차원축소를 고려하기 전에 훈련이 너무 느린지 먼저 원본 데이터로 시스템을 훈련시켜 본 후, 차선책으로 차원축소를 통해 시스템 속도를 높인다.
  
### 2) 이외의 차원축소 기법 : 매니폴드 학습(Manifold learning)

- 차원축소에서 투영이 언제나 최선의 방법이 아니다. 그 이유는 많은 경우에서 데이터들의 공간 모양이 **스위스 롤(Wsiss roll)**과 같이 부분 공간이 뒤틀리거나 휘어진 형태로 있을 수 있다.

![pca01](/assets/images/2020-09-22-pca01.png)

- 위 그림에서 볼 수 있듯, 주어진 데이터셋을 그대로 2D 평면으로 투영시키면 층이 뭉게져 클래스를 구별할 수 없게 된다.

- 이를 해결할 수 있는 방법으로 매니폴드 학습이 있다. 일반적으로 d-차원 매니폴드는 국소적으로 d-차원 초평면으로 볼 수 있는 n-차원 공간의 일부이다.(d < n) 스위스 롤은 d = 2이고 n = 3인, 국소적으로 2D 평면이지만 3차원으로 말려있는 데이터다. (추후에 추가 설명)

---

# 3. 수학적 설명

- PCA는 위에서 설명한 특이치 분해를 사용하므로 이후 설명은 특이치 분해 설명과 크게 다르지 않다. 다만, 아래 설명에서는 입력 데이터 $x$에 초점을 맞춰 다시 설명한다.

- $M$차원 데이터 $x$가 $N$개 있으면 이 데이터는 특징 행렬 $X \in R^{N \times M}$ 으로 나타낼 수 있다. 

- 이 데이터 내 정보를 가능한 손실이 적게 더 적은 차원인 $K(K < M)$ 차원의 차원축소 벡터로 $\hat{x}$로 **선형변환** 하고자 한다.

- 예를 들면 3차원 상의 데이터 집합을 2차원 평면에 투영하여 새로운 데이터 집합을 만들 때 어떤 평면을 선택해야 원래의 데이터와 투영된 데이터가 가장 비슷할까? (== 가장 차이가 적을까?) 이 평면을 찾는 문제와 같다.

- 설명을 단순하게 하기 위해 데이터가 원점을 중신으로 퍼져 있다고 가정한다.

- 데이터가 원점을 중심으로 존재하는 경우에는 벡터에 변환 행렬을 곱하는 연산으로 투영 벡터를 계산할 수 있다. 다음처럼 데이터 $x$에 변환 행렬 $W \in R^{N \times M}$을 곱해서 새로운 데이터 $\hat{x}_{i}$를 구하는 연산을 해보자.  
  
<center>
$\hat{x}_{i} = Wx_{i}$  
</center>
    
<center>
$x \in R^{M}, W \in R^{K \times M}, \hat{x} \in R^{K}$ 
</center>  
  
- 모든 데이터 $x_{i} (i = 1, 2, ..., N)$에 대해 변환하면 벡터가 아닌 행렬로 표현할 수 있다.

<center>
$\hat{X} = XW^{T}$    
</center>
  
<center>
$X \in R^{N \times M}, W^{T} \in R^{M \times K}, \hat{X} \in R^{N \times K}$ 
</center>   
  
- 이 식에서 행렬 $X$는 벡터 $x_{i} (i = 1, 2, ..., N)$을 행으로 가지는 행렬이고, 행렬 $\hat{X}$는 $\hat{x}_{i} (i = 1, 2, ..., N)$를 행으로 가지는 행렬이다. 

- 다시 말해, 입력 데이터 행렬 $X$가 행렬 $W$에 의해 선형변환되어 행렬 $\hat{X}$로 변환되었다.
  
- 이때, PCA의 목표는 변환 결과인 차원축소 벡터 $\hat{x} _{i}$ 정보가 원래 벡터인 $x _{i}$ 가졌던 정보와 가장 유사하게 되는 변환행렬 $W$를 찾는 것이다.  
  
- 그러나 $\hat{x}_ {i}$ 는 $K$차원 벡터로 원래의 $M$차원 벡터 $x_{i}$ 와 차원이 다르기 때문에 직접 두 벡터의 유사도를 비교할 수 없다.

- 따라서 $\hat{x}_{i}$를 다시 $M$차원 벡터로 선형변환하는 역변환행렬 $U \in R^{M \times K}$도 함께 찾아야한다. 그러면 원래의 데이터 벡터 $x$를 더 낮은 차원의 데이터로 변환했다가 다시 원래 차원으로 되돌릴 수 있다. 다시 $M$차원으로 재변환된 벡터를 $\hat{\hat{x}}$라 하자.

<center>
$\hat{\hat{x}} = U\hat{x}$  
</center>
  
<center>
$\hat{x} \in R^{K}, U \in R^{M \times K}, \hat{\hat{x}} \in R^{M}$
</center>  
  
- 이렇게 변환과 역변환 과정에서 원래 차원으로 되돌린 벡터 $U\hat{x}$ 는 원래의 벡터 $x$와 정확하게 같지 않다. 다만 이 값을 다시 한번 차원축소로 변환하면 다시 $\hat{x}$가 된다. 즉,  

<center>
$W\hat{\hat{x}} = WU\hat{x} = \hat{x}$
</center>  
  
- 따라서 W와 U는 다음 관계가 있다.  

<center>
$WU = I$
</center>
  
- 역변환행렬 $U$를 알고 있다고 가정하고 역변환 했을 때, 원래 벡터 $x$와 가장 비슷해지는 차원축소 벡터 $\hat{x}$를 다음과 같이 원본 데이터 $x$와 재변환된 $U \hat{x}$의 제곱 오차를 가장 적게하는 $\hat{x}$ 을 찾는다.

<center>
$\underset{\hat{x}}{argmin} ||x - U\hat{x}||^{2}$
</center>  
  
- 위의 목적함수는 다음과 같이 바꿀 수 있다.

<center>
$||x - U\hat{x}||^{2} = (x - U\hat{x})^{T}(x - U\hat{x})$
</center>
  
<center>
$ = x^{T}x - 2x^{T}U\hat{x} + \hat{x}^{T}\hat{x}$  
</center>    
  
- 목적함수를 최적화하기 위해 $\hat{x}$로 미분하여 식이 영벡터가 되는 값을 유도하면 아래와 같다.  
  
<center>
$\hat{x} = U^{T}x$
</center>

- 이 식을 원래 변환식과 비교하면, 아래와 같음을 알 수 있다.
  
<center>
$\hat{x} = Wx$  
</center>
  
<center>
$U = W^{T}$
</center>
  
- 따라서 다음이 성립하여 행렬 $W$는 직교행렬이 됨을 알 수 있다.

<center>
$WW^{T} = I$  
</center>  
  
- 남은 문제는 최적의 변환 행렬 $W$를 찾는 것이다. 이런 최적화 문제에서는 다음과 같이 총 제곱 오차가 가장 적은 $W$를 찾는 것으로 풀 수 있다.  

<center>
$\underset{w}{argmin} \sum_{i=1}^{N} ||x_{i} - W^{T}Wx_{i}||^{2}$
</center>  
  
- 모든 데이터에 적용하면 목적함수는 다음과 같다.  
  
<center>
$\underset{W}{argmin} ||X - XW^{T}W||^{2}$
</center> 
  
- 이 문제는 rank-K 근사문제이므로 $W$는 가장 큰 $K$개의 특이치에 대응하는 오른쪽 특이벡터($V^{T}$)로 만들어진 행렬이다.

---

# 참고

**1\.** [데이터 사이언스 스쿨 : 3.5 PCA](https://datascienceschool.net/view-notebook/f10aad8a34a4489697933f77c5d58e3a/)  

**2\.** Hands-On Machine Learning with Scikit-Learn & Tensorflow : 8장 차원축소  

**3\.** [공돌이의 수학정리노트 : 특이값 분해(SVD)](https://angeloyeo.github.io/2019/08/01/SVD.html)  