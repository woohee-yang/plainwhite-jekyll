---
title: "Attention Mechanism" ## 포스트 제목
category:       
    - Deep Learning
tags:          
    - attention
    - lstm
    - concept
comments:  true
use_math : true
last_modified_at : 2020-08-08
toc: true
# published: false
---

# 1. 개요

- seq2seq 모델들은 인코더(Encoder)에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축한다. 다른 말로 인코더를 통해 새로운 feature space로 입력 벡터를 사상시킨다.

- 디코더(Decoder)는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들어낸다.

- RNN 기반 seq2seq 모델들은 크게 2가지 문제가 있다.  
    **1\.** 하나의 고정된 크기의 벡터로 모든 정보를 압축하려 하니 정보 손실이 발생한다. -> RNN의 마지막 출력으로 정보 압축  
    **2\.** RNN의 고질적인 기울기 소실(Vanishing gradient) 문제가 존재한다.  

- 따라서 입력이 길어지면 길어질 수 록 출력 결과가 입력의 정보를 다 압축해내지 못해 성능이 점점 저하된다. 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스를 보정해주는 어텐션(Attention)이 등장하였다.

---

# 2. 어텐션(Attention)

- `어텐션의 기본 아이디어는 디코더에서 출력물을 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력을 다시 한 번 참고한다는 것이다.`

- `이때 전체 입력을 전부 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 출력과 연관이 있는 입력 부분을 집중(attention)해서 보게 된다.`

---

# 3. 어텐션 함수(Attention Function)

- 어텐션을 함수로 표현하면 아래와 같이 표현한다.

<center>$Attention(Q, K, V) = Attention Value$</center>

<center> $Q$ : 쿼리(Query), $K$ : 키(Key), $V$ : 값(Value) </center>

- 어텐션 함수는  
    **1\.** 주어진 쿼리에 대해 모든 키와의 유사도를 각각 구한다.  
    **2\.** 그리고 구해낸 유사도를 키와 사상되어 있는 각각의 값에 반영하여,  
    **3\.** 유사도가 반영된 값을 모두 더해서 리턴한다. 이를 어텐션 값(Attention value)라 하자.   

- 이제부터 설명할 seq2seq + attention 모델에서 Q, K, V 각각은 다음과 같다.
    + Q = Query : t 시점의 디코더 셀에서의 은닉 상태
    + K = Key : 모든 시점의 인코더 셀의 은닉 상태들
    + V = Values : 모든 시점의 인코더 셀의 은닉 상태들

![att01](/assets/images/2020-09-14-Attention01.png)

---

# 4. 내적 어텐션(Dot Product Attention)

- 어텐션은 다양한 종류가 있다. 그 중 기본이 되는 내적 어텐션을 살펴본다.

- seq2seq에서 사용되는 어텐션 중에서 내적 어텐션과 다른 어텐션의 차이는 주로 중간 수식의 차이고, 메커니즘은 거의 유사하다.

![att02](/assets/images/2020-09-14-Attention02.png)

- 위 그림은 디코더의 세번째 LSTM 셀에서 출력을 예측할 때, 어텐션 메커니즘을 사용하는 모습을 보여준다. 

- 여기서 디코더의 첫 번째, 두 번째 LSTM 셀은 이미 어텐션 메커니즘을 통해 je와 suis를 예측하는 과정을 거쳤다고 가정한다. 그리고 디코더의 세 번째 LSTM 셀은 출력 단어를 예측하기 위해 인코더의 모든 입력 단어들의 정보를 다시 한 번 참고하고자 한다. 

- 여기서 주목해야 할 것은 인코더의 소프트맥스(softmax) 함수다.

- 소프트맥스 함수를 통해 나온 결과값은 I, am, a, student 단어 각각이 출력 단어를 예측할 때 얼마나 도움이 되는지 정도를 수치화 한 값이다.

- 위 그림에서 빨간 직사각형의 크기로 소프트맥스 함수의 결과값 크기가 표현되어 있다. (직사각형 크기 = 소프트맥스 출력값 = 확률 = 출력 예측 도움 수치)

- 각 입력 단어가 디코더의 예측에 도움이 되는 정도가 수치화되어 측정되면 이를 하나의 정보로 담아서 디코더로 전송한다. 위의 그림에서는 초록색 삼각형이 이에 해당된다.

- 결과적으로, 디코더는 출력 단어를 더 정확하게 예측할 확률이 높아진다.

---

## 4.1 어텐션 스코어(Attention Score) 구하기

![att03](/assets/images/2020-09-14-Attention03.png)

- 인코더 시점을 각각 1,2,...,N이라 하였을 때, 인코더의 은닉 상태(hidden state)를 각각 $h_{1},h_{2},...,h_{N}$ 라고 하자.

- 디코더의 현재 시점 t에서 디코더의 은닉 상태는 $s_{t}$라고 하자.

- 여기서 인코더의 은닉 상태와 디코더의 은닉 상태 차원은 같다고 가정한다. 따라서 위의 그림의 경우 인코더의 은닉 상태와 디코더의 은닉 상태가 4차원으로 동일하다.

- 먼저 디코더의 현재 시점 t에서 필요한 입력값을 생각해보자. 시점 t에서 출력 단어를 예측하기 위해서 디코더의 셀은 두개의 입력값을 필요로 한다.  
    **1\.** 이전 시점 t-1의 은닉 상태  
    **2\.** 이전 시점 t-1의 출력 값  


- 어텐션에서는 여기에 더불어 `3. 어텐션 값(Attention value)` 이 필요하다. t번째 단어를 예측하기 위한 어텐션 값을 $a_{t}$라 정의하자. $a_{t}$를 구하기 위해, 먼저 어텐션 스코어(Attention score)를 구한다.

- `어텐션 스코어란, 현재 디코더의 시점 t에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 $s_{t}$와 얼마나 유사한지 판단하는 값이다.`

- 내적 어텐션을 이 스코어를 구하기 위해 $s_{t}$를 전치(transpose)하고 각 은닉 상태와 내적(dot product)을 수행한다. 따라서 모든 어텐션 스코어 값은 스칼라이다.

![att04](/assets/images/2020-09-14-Attention04.png)

<center>$score(s_{t}, h_{i}) = s^{T}_{t} h_{i}$</center>

- $s_{t}$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값을 $e^{t}$라 정의하자.

<center>$e^{t} = [s^{T}_{t}h_{1}, ..., s^{T}_{t}h_{N}]$</center>

---

## 4.2 소프트맥스 함수를 통해 어텐션 분포(Attention Distribution) 구하기

![att05](/assets/images/2020-09-14-Attention05.png)

- $e^{t}$에 소프트맥스 함수를 적용하여, 모든 값을 합하면 1이 되는 확률 분포를 구하자. 이를 어텐션 분포(Attention distribution)이라 하며, 각각의 값은 어텐션 가중치(Attention weight)라 한다.

- 예를 들어, 소프트맥스 함수를 적용하여 얻은 출력값인 I, am, a, student의 어텐션 가중치를 각각 0.1, 0.4, 0.1, 0.4라 하자. 이들의 합은 1이 된다.

- 디코더 시점 t에서의 어텐션 가중치의 모음값인 어텐션 분포, $\alpha_{t}$는 다음과 같다.

<center>$\alpha_{t} = softmax(e^{t})$</center>

---

## 4.3 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값(Attention Value) 구하기

![att06](/assets/images/2020-09-14-Attention06.png)

- 이제 지금까지 얻은 정보를 하나로 합치는 단계이다. 어텐션의 최종 결과값을 얻기 위해 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고, 최종적으로 모두 더한다. 따라서 가중합(Weighted sum)을 구한다.

- 어텐션의 최종 결과인 어텐션 함수의 출력값, 어텐션 값(attention value) $\alpha_{t}$는 아래와 같다.

<center>$\alpha^{t} = \sum^{N}_{i=1} \alpha^{t}_{i}h_{i}$</center>

- 이러한 어텐션 값 $\alpha_{t}$는 종종 인코더의 문맥을 포함하고 있다하여, *컨텍스트 벡터(context vector)* 라고도 불린다.

---

## 4.4 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결(Concatenate)

![att07](/assets/images/2020-09-14-Attention07.png)

- 어텐션 값이 구해지면 어텐션 메커니즘은 $\alpha_{t}$를 $s_{t}$와 결합하여 하나의 벡터로 만든다.(단순 concatenation) 이를 $v_{t}$라 하자.

- 그리고 $v_{t}$를 $\hat{y}$ 예측 연산의 입력으로 사용하여 인코더에서 얻은 정보를 활용하므로 $\hat{y}$를 좀 더 잘 예측할 수 있게 된다. 이것이 어텐션 메커니즘의 핵심이다.

---

## 4.5 출력층 연산의 입력이 되는 $\tilde{s_{t}}$를 계산


![att08](/assets/images/2020-09-14-Attention08.png)

- 원 논문에서는 $v_{t}$를 바로 출력층으로 보내기 전, 신경망 연산을 한 번 더 추가하였다.

- 가중치 행렬과 곱한 후 tanh 함수를 지나도록 하여 출력층 연산을 위한 새로운 벡터 $\tilde{s_{t}}$를 얻었다.

- 어텐션 메커니즘을 사용하지 않는 seq2seq에서 출력층의 입력이 t시점의 은닉 상태인 $s_{t}$였던 반면, 어텐션 메커니즘에서는 출력층의 입력이 $\tilde{s_{t}}$가 되는 것이다. 이를 식으로 표현하면 아래와 같다.

<center>$\tilde{s_{t}} = tanh(W_{c}|\alpha_{t};s_{t}| + b_{c})$ </center>

<center>$W_{c}$ : 학습 가능한 가중치 행렬, $b_{c}$ : 편향 (그림에는 생략)</center>

---

## 4.6 $\tilde{s_{t}}$를 출력층의 입력으로 사용

- $\tilde{s_{t}}$를 출력층의 입력으로 사용하여 예측 벡터를 얻는다.

<center> $\hat{y_{t}} = Softmax(W_{s}\tilde{s_{t}} + b_{y})$ </center>


---

# 5. 다양한 종류의 어텐션(Attention)

- 내적 어텐션과 다른 어텐션들의 차이는 어텐션 스코어 함수이다. 어텐션 스코어를 구하는 방법은 여러가지가 제시되었고, 현재 제시된 대표적 어텐션 스코어는 아래와 같다.

![att09](/assets/images/2020-09-14-Attention09.PNG)

- Notations
    + $s_{t}$ : query
    + $h_{t}$ : keys
    + $W_{a}, $W_{b}$ : learnable weight matrix


---

## 참고

1. [딥 러닝을 이용한 자연어 처리 입문 : 16. 어텐션 메커니즘](https://wikidocs.net/22893)
2. [기초부터 시작하는 NLP: SEQUENCE TO SEQUENCE 네트워크와 ATTENTION을 이용한 번역](https://tutorials.pytorch.kr/intermediate/seq2seq_translation_tutorial.html#nlp-sequence-to-sequence-attention)
    




