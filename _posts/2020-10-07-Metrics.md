---
title: "Metrics" ## 포스트 제목
category:       
    - Basic
tags:           
    - Accuracy
    - AUC
    - Recall
    - Precision
    - F1 score
    - Kappa
    - MCC
    - MAE
    - MSE
    - RMSE
    - R2
    - RMSLE
    - MAPE
comments:  true
use_math : true
last_modified_at : 2020-10-07
toc: true
---

# 1. 소개

- 머신러닝의 분류와 회귀 문제에서 사용하는 여러 모델 평가 지표들에 대해 정리한다.

- 머신러닝에서는 주어진 문제들에 맞추어 모델 평가 지표들이 달라진다. 예를 들어, 여러개 클래스 분류 문제라면 분류가 잘 되었는지 안되었는지 알아보기 위해 정확도를 측정한다. 이 때, 정확도를 측정하는 방법은 모델을 평가하고 싶은 방향에 따라 다르다. 원래 정답을 정확하게 맞췄는지 알고 싶어 정답을 맞춘 비율만 조사하거나, 틀린 비율이 더 알고 싶어 틀린 비율만 계산하는 등 필요에 따라 여러 가지가 있을 수 있다. 

- 아래 분류된 평가 지표들은 `pycaret` AutoML 라이브러리에서 사용할 수 있다.  
    + Classification : Accuracy / AUC / Recall / Precision / F1 score / Kappa / MCC  
    + Regression : MAE / MSE / RMSE / R2 / RMSLE / MAPE  
    + 이외 : mAP / IoU (object detection) 등

---

# 2. 분류

## 2.1 4 Cases

분류 문제에서 정확도는 모델이 실제 정답과 맞게 잘 분류하였는지 아닌지가 가장 중요하다. 이진분류의 경우, 조금만 생각해보면 모델이 한 샘플에 대해 내놓을 수 있는 결론은 True / False 두 가지이다. 이는 실제 정답의 경우도 한 샘플에 대해서 A 카테고리에 속하느냐 아니냐 두 가지로 나눌 수 있다. 따라서 실제 정답과 모델 예측 간의 관계는 아래 도표와 같이 4가지 케이스로 나눌 수 있다.  

![metrics01](/assets/images/2020-10-07-metrics01.PNG)  

각 케이스를 살펴보자.  

- True Positive(TP) : 실제 True인 정답 / 모델이 True라고 예측 (정답)
- False Positive(FP) : 실제 False인 정답 / 모델이 True라고 예측 (오답)
- False Negative(FN) : 실제 True인 정답 / 모델이 False라고 예측 (오답)
- True Negative(TN) : 실제 False인 정답 / 모델이 False라고 예측 (정답)

  
위 케이스들에서 보면, 단순하게 생각하는 정답률은 TP 뿐일 것이다. 하지만 TN의 케이스도 정답이므로 우리는 상황과 문제에 맞게 모든 정답을 고려할 것인지 아닌지 어떻게 고려하여 모델을 평가할 것인지 적절하게 판단해야한다. 머신러닝에서는 이에 맞게 몇 가지 평가 지표들이 존재한다.

---

## 2.2 Precision / Recall / Accuracy / F1 score

앞서 설명한 4가지 케이스들을 조합하여 간단하게 정의된 평가 지표들이다. 각 지표들의 의미와 활용 사례 또는 단점에 대해 알아보자. 여기서는 한달 동안의 일기예보 예측 문제가 주어졌다고 가정하자. 날씨는 비 또는 맑음 이 두가지로만 예측한다.

### 2.1 Precision(정밀도)

`정밀도`는 `모델이 True라고 분류한 것 중, 실제 True인 것의 비율`이다.

<center>
$(Precision) = \frac{TP}{TP+FP}$
</center>
  
- 다시 말해, 모델이 True라고 분류한 FP와 TP 케이스들 중 실제 True 정답을 맞춘 TP의 비율이다. 

- 이는 **Positive 정답율, PPV(Positive Predictive Value)** 라고도 불린다. 예제 상황에서는 모델이 날씨가 맑다라고 예측했을때, 실제 날씨도 맑았는지 확인하는 지표이다.  
  
- 이 지표는 단순 카운팅에 기반하므로 실제 정답을 맞췄지만 모델이 항상 특정 클래스 하나로만 출력한다면 정밀도는 0에 가까워진다. 

- 예를 들어, 모델이 한 달, 30일 동안 모든 날을 맑음으로 예측했을때, 실제로는 단 20일만 맑았다면 정밀도는 20/30이 된다. 따라서 모델의 관점에서는 예측이 20/30에 비율로 맞았다는 뜻이다.

### 2.2 Recall(재현율)

`재현율`은 `실제 True인 것 중 모델이 True라고 예측한 것의 비율`이다.

<center>
$(Recall) = \frac{TP}{TP+FN}$  
</center>
  
- 통계학에서는 **sensitivity**, 다른 분야에서는 **hit rate**라고 불린다.

- 이는 정밀도와 상반된 관점으로 TP를 바라본다. 정밀도나 재현율 모두 실제 True인 정답을 모델이 True라고 예측한 것을 알고 싶어 하나, 정밀도는 모델 관점에서, 재현율은 실제 정답 관점에서 해석한다.

- 정밀도에서 들었던 예시와 마찬가지로, 모델이 한 달, 30일동안 모든 날을 맑았다고 예측했을때, 실제론는 20일만 맑았다면, 정밀도는 20/30(TP=14, FP=16)이 되지만 재현율은 20/20(TP=20, FN=0) = 1이 된다. 즉, 재현율에 의하면 현재 모델은 100%로 아주 훌륭한 모델이 되어버린다.

- 하지만, 두 지표를 놓고 비교해 보면 현재 모델은 예측을 포기하고 맑은 날이 좀 더 많기 때문에 전부 맑다고 예측해버린 제대로 못 배운 모델이다. 쉽게 말해 까짓거 맑은 날이 더 많은 계절인데 그냥 전부 맑다고 하자 라고 과도한 낙관주의가 되어버려 오로지 재현율만 극도로 높여버린 상황이다. 그러면 우리는 비가 오는 나머지 10일은 늘 우산을 제때 챙기지 못해 물에 빠진 생쥐꼴이 될 수 있다. 이는 절대로 이상적인 모델이라 할 수 없을 것이다.

- 따라서, 우리는 실제로 비가 온날 10일들 중에서 예측한 날이 얼마나 되는지도 생각하여 재현율 뿐만 아니라 정밀도도 같이 높일 수 있는 방향으로 모델을 학습시켜야 한다. 정밀도와 재현율을 함께 고려하면 실제 비가 온 날들 중 우리 모델이 비가 왔다고 예측한 비율을 같이 참고하여 제대로 평가할 수 있다. 즉, 정밀도와 재현율은 상호보완적인 지표들이며 이 둘을 함께 고려해야 좋은 모델이 된다.

#### Precision-Recall Trade-off

---

## 2.3 Accuracy

---


## 2.4 ROC & AUC

---

## 2.5 Kappa

---

## 2.6 MCC

---

# 참고

**1\.** [조재성님 티스토리 : 15.ROC와 AUC](https://nittaku.tistory.com/297)